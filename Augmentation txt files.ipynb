{
 "metadata": {
  "name": "",
  "signature": "sha256:1c8433c53a2b36f6173451b02ab7fdf7db9ac2e8544f9cd4d0d78b729a9eeda0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Augmented training data\n",
      "# Just augment the folds\n",
      "import numpy as np\n",
      "import os\n",
      "\n",
      "# for mode in ['test', 'train']:\n",
      "for mode in ['train']:\n",
      "#     for n in range(5):\n",
      "    for n in ['_all']:\n",
      "        # text_file = 'test0.txt'\n",
      "        text_file = mode + str(n) + '.txt'\n",
      "        print 'Converting:', text_file\n",
      "        \n",
      "        f_path = os.path.join('./data', text_file)\n",
      "        aug_train_path = '../../data/ndsb/augment/train'\n",
      "        save_path = os.path.join('./data/augmented', text_file)\n",
      "#         save_path = os.path.join('./data/augmented', 'unlabeled' + text_file)\n",
      "        angs = range(0, 360, 45) \n",
      "        suffixes = np.array(['_rot' + str(ang) for ang in angs])\n",
      "\n",
      "        arr = np.loadtxt(f_path, dtype=str, delimiter='\\t')\n",
      "\n",
      "        # Could do some weird thing with meshgrid, but I am too tired\n",
      "        with open(save_path, 'wb') as f_save:\n",
      "            for ii, (entry, label) in enumerate(arr):\n",
      "                d, b = os.path.split(entry)\n",
      "                l_str = os.path.basename(d)\n",
      "                b = os.path.splitext(b)[0]\n",
      "                for suf in suffixes:\n",
      "                    new_path = os.path.abspath(os.path.join(aug_train_path, l_str, b + suf + '.jpg'))\n",
      "                    line = new_path + '\\t' + label + '\\n'\n",
      "#                     line = new_path + '\\t' + '-1' + '\\n'\n",
      "                    f_save.write(line)\n",
      "\n",
      "                if (ii%1000 == 0):\n",
      "                    print ii, 'written'\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Converting: train_all.txt\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "2000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "3000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "4000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "5000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "6000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "7000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "8000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "9000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "10000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "11000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "12000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "13000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "14000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "15000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "16000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "17000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "18000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "19000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "20000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "21000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "22000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "23000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "24000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "26000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "27000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "28000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "29000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n",
        "30000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " written\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.path.basename(d)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "'unknown_unclassified'"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import os\n",
      "import glob\n",
      "\n",
      "aug_test_path = '/data/ndsb/augment/test'\n",
      "orig_test_path = '/data/ndsb/test'\n",
      "save_path = os.path.join('/data/ndsb/augment/test_aug.txt')\n",
      "if 1:\n",
      "    files_aug = [os.path.abspath(f) for f in glob.glob(os.path.join(aug_test_path, '*', '*.jpg'))]\n",
      "    files_orig = [os.path.abspath(f) for f in glob.glob(os.path.join(orig_test_path, '*.jpg'))]\n",
      "    files_all = np.r_[files_aug, files_orig]\n",
      "    y = -np.ones(len(files_all), dtype=int)\n",
      "    np.savetxt(save_path, np.c_[files_all, y], fmt='%s', delimiter='\\t')\n",
      "    \n",
      "    print save_path, 'saved'\n",
      "    \n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/data/ndsb/augment/test/test_aug.txt saved\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(files_aug)\n",
      "print len(files_orig)\n",
      "print len(files_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "451045\n",
        "64435\n",
        "515480\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#######################################\n",
      "import numpy as np\n",
      "import os\n",
      "import glob\n",
      "from sklearn.cross_validation import StratifiedKFold\n",
      "from tools.le import le\n",
      "\n",
      "TRAIN_PATH = '/media/raid_arr/data/ndsb/train/'\n",
      "TRAIN_AUG_PATH = '/media/raid_arr/data/ndsb/augment/train/'\n",
      "\n",
      "labels = os.listdir(TRAIN_PATH)\n",
      "files_all = np.array([os.path.abspath(f) for f in glob.glob(os.path.join(TRAIN_PATH, '*', '*.jpg'))])\n",
      "y_str = [os.path.basename(os.path.dirname(f)) for f in files_all]\n",
      "y = le.transform(y_str)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SAVE_PATH = '/media/raid_arr/data/ndsb/folds'\n",
      "k = 5\n",
      "skf = StratifiedKFold(y, n_folds=k, shuffle=True, random_state=0)\n",
      "\n",
      "save_fn = lambda name, f_mode, y_mode: np.savetxt(\n",
      "        os.path.join(SAVE_PATH, str(name) + '.txt'), \n",
      "        np.c_[f_mode, y_mode], \n",
      "        fmt='%s', delimiter='\\t')\n",
      "\n",
      "angs = range(0, 360, 45) \n",
      "suffixes = np.array(['_rot' + str(ang) for ang in angs])\n",
      "aug_name = lambda f_i, suff: os.path.join(TRAIN_AUG_PATH, os.path.basename(os.path.dirname(f_i)),\n",
      "                                          os.path.splitext(os.path.basename(f_i))[0] + \n",
      "                                          suff + os.path.splitext(os.path.basename(f_i))[1])\n",
      "\n",
      "\n",
      "save_fn('train_all', files_all, y)\n",
      "for fold_ii, (train_ind, test_ind) in enumerate(skf):\n",
      "    f_train, f_test = files_all[train_ind], files_all[test_ind]\n",
      "    y_train, y_test = y[train_ind], y[test_ind]\n",
      "    save_fn('train' + str(fold_ii), f_train, y_train)\n",
      "    save_fn('test' + str(fold_ii) , f_test, y_test)\n",
      "    \n",
      "    # Aug\n",
      "    f_train_aug = np.array([aug_name(f_i, suff) for f_i in f_train for suff in suffixes])\n",
      "    y_train_aug = np.array([y_i for y_i in y_train for suff in suffixes])\n",
      "    save_fn('train_aug' + str(fold_ii), f_train_aug, y_train_aug)\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.path.basename(os.path.dirname(f_i))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "'copepod_calanoid_eggs'"
       ]
      }
     ],
     "prompt_number": 50
    }
   ],
   "metadata": {}
  }
 ]
}
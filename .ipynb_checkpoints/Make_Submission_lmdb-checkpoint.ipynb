{
 "metadata": {
  "name": "",
  "signature": "sha256:894ab4a20716bd476ba3e71cef7cd3769780ac2ff8c2914bc9f398ee588c81ac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Try predicting the augmented versions as well\n",
      "# aggregate the results from all augmentations of 1 image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import time\n",
      "import sys\n",
      "import tools.my_io as my_io\n",
      "import caffe\n",
      "import os\n",
      "import pickle\n",
      "import itertools\n",
      "\n",
      "mode = 'normal'\n",
      "# mode = 'aug'\n",
      "\n",
      "# Set the right path to your model definition file, pretrained model weights,\n",
      "# and the image you would like to classify.\n",
      "MODEL_FILE = './deploy_vanilla.prototxt'\n",
      "# PRETRAINED = './models/vanilla/vanilla_iter_20000.caffemodel'\n",
      "PRETRAINED = '/media/raid_arr/data/ndsb/models/pl_iter_60000.caffemodel'\n",
      "MEAN_FILE = '/media/raid_arr/data/ndsb/augment/testaug_mean.npy'\n",
      "# TEST_FILE = './data/test_final.txt'\n",
      "\n",
      "if mode == 'aug':\n",
      "    TEST_DB = '/data/ndsb/augment/ndsb_testaug_lmdb/'\n",
      "else:\n",
      "    TEST_DB = '/data/ndsb/ndsb_test_lmdb'\n",
      "\n",
      "N = 20000   # Chunk size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # Loading From Database\n",
      "# print 'Loading data...'\n",
      "# tic = time.time()\n",
      "# data = my_io.load_lmdb(TEST_DB)\n",
      "# print \"Done in %.2f s.\" % (time.time() - tic)\n",
      "\n",
      "# # test_files_all, images, labels = zip(*data)\n",
      "# # test_labels = labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# image_dims = data[0][1].shape[:2]\n",
      "image_dims = (64, 64)\n",
      "net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
      "                       mean=np.load(MEAN_FILE),\n",
      "                       raw_scale=1.0,    # 255 if load from caffe.io, 1.0 if load from my_io lmdb\n",
      "                       image_dims=image_dims,\n",
      "                       gpu=True)\n",
      "caffe.set_phase_test()\n",
      "caffe.set_mode_gpu()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# PREDICTION TIME\n",
      "print 'Predicting...', TEST_DB\n",
      "prediction_list = []\n",
      "test_files_list = []\n",
      "next_key = ''\n",
      "first_run = True\n",
      "while next_key or first_run:\n",
      "    first_run = False\n",
      "    print 'Starting at key: ', next_key\n",
      "    read_start = time.time()\n",
      "    data_chunk, next_key = my_io.load_lmdb_chunk(TEST_DB, next_key, N)\n",
      "    print \"Read done in %.2f s.\" % (time.time() - read_start)\n",
      "    print 'Chunk size:', len(data_chunk)\n",
      "    sys.stdout.flush()\n",
      "    pred_start = time.time()\n",
      "    test_files_chunk, images_chunk, fake_labels = zip(*data_chunk)\n",
      "    prediction = net.predict(images_chunk)\n",
      "#     prediction = np.array([1]) # for testing db read\n",
      "    prediction_list.append(prediction)\n",
      "    test_files_list.append(test_files_chunk)\n",
      "    print \"Pred done in %.2f s.\" % (time.time() - pred_start)\n",
      "    sys.stdout.flush()\n",
      "    \n",
      "predictions = np.concatenate(prediction_list)\n",
      "test_files = list(itertools.chain(*test_files_list))\n",
      "print \"Done predicting\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Predicting... /data/ndsb/ndsb_test_lmdb\n",
        "Starting at key: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " \n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 671.26 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00020000_/data/ndsb/test/143698.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 622.56 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00040000_/data/ndsb/test/9227.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 620.37 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00060000_/data/ndsb/test/74795.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 620.31 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00080000_/data/ndsb/test/56438.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 612.43 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00100000_/data/ndsb/test/64677.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 620.75 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00120000_/data/ndsb/test/145818.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10400\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 321.81 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done predicting\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Saving predictions\n",
      "# pred_save_path = '/media/raid_arr/data/ndsb/saved_preds/pred.p'\n",
      "# pickle.dump(predictions, open(pred_save_path, 'wb'))\n",
      "# print 'Saved predictions:', pred_save_path\n",
      "\n",
      "# # Saving test_file_paths\n",
      "# test_f_save_path = '/media/raid_arr/data/ndsb/saved_preds/test_files.p'\n",
      "# pickle.dump(test_files, open(test_f_save_path, 'wb'))\n",
      "# print 'Saved predictions:', test_f_save_path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if mode == 'aug':\n",
      "    # Averaging over predictions for augmentations\n",
      "    test_files_arr = np.array(test_files)\n",
      "    # Setting up base names\n",
      "    base_im = lambda f_path: os.path.basename(f_path).split('_')[0]\n",
      "    start = time.time()\n",
      "    base_names = np.array([base_im(f_path) for f_path in test_files_arr])\n",
      "    print \"Base names done in %.2f s.\" % (time.time() - start)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if mode == 'aug':\n",
      "    unique_im_names = list(set(base_names))\n",
      "    predictions_agg = np.zeros((len(unique_ims), predictions.shape[1]))\n",
      "    for ii, im_name in enumerate(unique_im_names):\n",
      "        inds = base_names == im_name\n",
      "        p_mean = np.mean(predictions[inds, :], axis=0)\n",
      "        predictions_agg[ii, :] = p_mean\n",
      "    print 'Finished Aggregation'\n",
      "    unique_im_f = [im_n + '.jpg' for im_n in unique_im_names]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SUBMISSION CREATION\n",
      "# test_files_all, images, labels = zip(*data)\n",
      "import tools.submission as sub\n",
      "f_name='SUBMISSION_PL60000_augagg.csv'\n",
      "if mode == 'aug':\n",
      "    sub.make_submission(unique_im_f, predictions_agg, f_name=f_name)\n",
      "else:\n",
      "    \n",
      "    sub.make_submission(test_files_all, predictions, f_name=f_name)\n",
      "\n",
      "print 'Submission created:', f_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'data' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-95-f23ac05d973d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_submission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_im_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions_agg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtest_files_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0msub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_submission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_files_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# np.sum(predictions_agg)\n",
      "# predictions_agg.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
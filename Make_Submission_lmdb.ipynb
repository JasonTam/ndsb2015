{
 "metadata": {
  "name": "",
  "signature": "sha256:8fa70aad03d48b0e87cc420ac95857566bfac18099a0ff99ed2f3a25861cfe5d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Try predicting the augmented versions as well\n",
      "# aggregate the results from all augmentations of 1 image\n",
      "\n",
      "# Average/weighted average the predictions from an ensemble of nets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import time\n",
      "import sys\n",
      "import tools.my_io as my_io\n",
      "import caffe\n",
      "import os\n",
      "import pickle\n",
      "import itertools\n",
      "from scipy.misc import imresize\n",
      "\n",
      "mode = 'normal'\n",
      "# mode = 'aug'\n",
      "\n",
      "# Set the right path to your model definition file, pretrained model weights,\n",
      "# and the image you would like to classify.\n",
      "# MODEL_FILE = './deploy_vanilla.prototxt'\n",
      "# MODEL_FILE = './deploy_deeper.prototxt'\n",
      "MODEL_FILE = '/media/raid_arr/data/ndsb/config/deploy_shallow_googlenet.prototxt'\n",
      "# PRETRAINED = './models/vanilla/vanilla_iter_20000.caffemodel'\n",
      "# PRETRAINED = '/media/raid_arr/data/ndsb/models/pl_iter_60000.caffemodel'\n",
      "PRETRAINED = '/media/raid_arr/data/ndsb/models/shallow_gnet_iter_88000.caffemodel'\n",
      "MEAN_FILE = '/media/raid_arr/data/ndsb/augment/testaug_mean.npy'\n",
      "# TEST_FILE = './data/test_final.txt'\n",
      "\n",
      "if mode == 'aug':\n",
      "    TEST_DB = '/data/ndsb/augment/ndsb_testaug_lmdb/'\n",
      "else:\n",
      "    TEST_DB = '/data/ndsb/ndsb_test_lmdb'\n",
      "\n",
      "N = 20000   # Chunk size\n",
      "model_name = os.path.splitext(os.path.basename(PRETRAINED))[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # Loading From Database\n",
      "# print 'Loading data...'\n",
      "# tic = time.time()\n",
      "# data = my_io.load_lmdb(TEST_DB)\n",
      "# print \"Done in %.2f s.\" % (time.time() - tic)\n",
      "\n",
      "# # test_files_all, images, labels = zip(*data)\n",
      "# # test_labels = labels\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# image_dims = data[0][1].shape[:2]\n",
      "caffe.set_mode_gpu()\n",
      "# caffe.set_phase_test()\n",
      "image_dims = (64, 64)\n",
      "crop_dims = np.array([57, 57])\n",
      "\n",
      "mean=np.load(MEAN_FILE)\n",
      "mean.shape\n",
      "mean_resized = caffe.io.resize_image(mean.transpose((1,2,0)), crop_dims).transpose((2,0,1))\n",
      "\n",
      "net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
      "                       mean=mean_resized,\n",
      "                       raw_scale=1.0,    # 255 if load from caffe.io, 1.0 if load from my_io lmdb\n",
      "                       image_dims=image_dims)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# PREDICTION TIME\n",
      "print 'Predicting...', TEST_DB\n",
      "prediction_list = []\n",
      "test_files_list = []\n",
      "next_key = ''\n",
      "first_run = True\n",
      "while next_key or first_run:\n",
      "    first_run = False\n",
      "    print 'Starting at key: ', next_key\n",
      "    read_start = time.time()\n",
      "    data_chunk, next_key = my_io.load_lmdb_chunk(TEST_DB, next_key, N)\n",
      "    print \"Read done in %.2f s.\" % (time.time() - read_start)\n",
      "    print 'Chunk size:', len(data_chunk)\n",
      "    sys.stdout.flush()\n",
      "    pred_start = time.time()\n",
      "    test_files_chunk, images_chunk, fake_labels = zip(*data_chunk)\n",
      "    prediction = net.predict(images_chunk)\n",
      "#     prediction = np.array([1]) # for testing db read\n",
      "    prediction_list.append(prediction)\n",
      "    test_files_list.append(test_files_chunk)\n",
      "    print \"Pred done in %.2f s.\" % (time.time() - pred_start)\n",
      "    sys.stdout.flush()\n",
      "    \n",
      "predictions = np.concatenate(prediction_list)\n",
      "test_files = list(itertools.chain(*test_files_list))\n",
      "print \"Done predicting\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Predicting... /data/ndsb/ndsb_test_lmdb\n",
        "Starting at key:  \n",
        "Read done in 8.63 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 1290.48 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00020000_/data/ndsb/test/143698.jpg\n",
        "Read done in 2.75 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 1228.88 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00040000_/data/ndsb/test/9227.jpg\n",
        "Read done in 2.60 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 1228.07 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00060000_/data/ndsb/test/74795.jpg\n",
        "Read done in 2.59 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 1228.20 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00080000_/data/ndsb/test/56438.jpg\n",
        "Read done in 2.57 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 1228.59 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00100000_/data/ndsb/test/64677.jpg\n",
        "Read done in 2.52 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 20000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 1227.89 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00120000_/data/ndsb/test/145818.jpg\n",
        "Read done in 1.30 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 10400\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 638.56 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done predicting\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Saving predictions\n",
      "# pred_save_path = '/media/raid_arr/data/ndsb/saved_preds/pred.p'\n",
      "# pickle.dump(predictions, open(pred_save_path, 'wb'))\n",
      "# print 'Saved predictions:', pred_save_path\n",
      "\n",
      "# # Saving test_file_paths\n",
      "# test_f_save_path = '/media/raid_arr/data/ndsb/saved_preds/test_files.p'\n",
      "# pickle.dump(test_files, open(test_f_save_path, 'wb'))\n",
      "# print 'Saved predictions:', test_f_save_path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if mode == 'aug':\n",
      "    # Averaging over predictions for augmentations\n",
      "    test_files_arr = np.array(test_files)\n",
      "    # Setting up base names\n",
      "    base_im = lambda f_path: os.path.basename(f_path).split('_')[0]\n",
      "    start = time.time()\n",
      "    base_names = np.array([base_im(f_path) for f_path in test_files_arr])\n",
      "    print \"Base names done in %.2f s.\" % (time.time() - start)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if mode == 'aug':\n",
      "    unique_im_names = list(set(base_names))\n",
      "    predictions_agg = np.zeros((len(unique_ims), predictions.shape[1]))\n",
      "    for ii, im_name in enumerate(unique_im_names):\n",
      "        inds = base_names == im_name\n",
      "        p_mean = np.mean(predictions[inds, :], axis=0)\n",
      "        predictions_agg[ii, :] = p_mean\n",
      "    print 'Finished Aggregation'\n",
      "    unique_im_f = [im_n + '.jpg' for im_n in unique_im_names]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SUBMISSION CREATION\n",
      "# test_files_all, images, labels = zip(*data)\n",
      "import tools.submission as sub\n",
      "f_name='SUBMISSION_%s.csv' % model_name\n",
      "if mode == 'aug':\n",
      "    sub.make_submission(unique_im_f, predictions_agg, f_name=f_name)\n",
      "else:\n",
      "    \n",
      "    sub.make_submission(test_files, predictions, f_name=f_name)\n",
      "\n",
      "print 'Submission created:', f_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Submission created: SUBMISSION_PL_shallow_gnet_iter_88000.csv\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# np.sum(predictions_agg)\n",
      "# predictions_agg.shape\n",
      "# test_files\n",
      "print 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k, v in net.blobs.items():\n",
      "    print k, v.data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data (10, 1, 57, 57)\n",
        "conv1/7x7_s2 (10, 64, 29, 29)\n",
        "pool1/3x3_s2 (10, 64, 14, 14)\n",
        "pool1/norm1 (10, 64, 14, 14)\n",
        "conv2/3x3_reduce (10, 64, 14, 14)\n",
        "conv2/3x3 (10, 192, 14, 14)\n",
        "conv2/norm2 (10, 192, 14, 14)\n",
        "pool2/3x3_s2 (10, 192, 7, 7)\n",
        "pool2/3x3_s2_pool2/3x3_s2_0_split_0 (10, 192, 7, 7)\n",
        "pool2/3x3_s2_pool2/3x3_s2_0_split_1 (10, 192, 7, 7)\n",
        "pool2/3x3_s2_pool2/3x3_s2_0_split_2 (10, 192, 7, 7)\n",
        "pool2/3x3_s2_pool2/3x3_s2_0_split_3 (10, 192, 7, 7)\n",
        "inception_3a/1x1 (10, 64, 7, 7)\n",
        "inception_3a/3x3_reduce (10, 96, 7, 7)\n",
        "inception_3a/3x3 (10, 128, 7, 7)\n",
        "inception_3a/5x5_reduce (10, 16, 7, 7)\n",
        "inception_3a/5x5 (10, 32, 7, 7)\n",
        "inception_3a/pool (10, 192, 7, 7)\n",
        "inception_3a/pool_proj (10, 32, 7, 7)\n",
        "inception_3a/output (10, 256, 7, 7)\n",
        "inception_3a/output_inception_3a/output_0_split_0 (10, 256, 7, 7)\n",
        "inception_3a/output_inception_3a/output_0_split_1 (10, 256, 7, 7)\n",
        "inception_3a/output_inception_3a/output_0_split_2 (10, 256, 7, 7)\n",
        "inception_3a/output_inception_3a/output_0_split_3 (10, 256, 7, 7)\n",
        "inception_3b/1x1 (10, 128, 7, 7)\n",
        "inception_3b/3x3_reduce (10, 128, 7, 7)\n",
        "inception_3b/3x3 (10, 192, 7, 7)\n",
        "inception_3b/5x5_reduce "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(10, 32, 7, 7)\n",
        "inception_3b/5x5 (10, 96, 7, 7)\n",
        "inception_3b/pool (10, 256, 7, 7)\n",
        "inception_3b/pool_proj (10, 64, 7, 7)\n",
        "inception_3b/output (10, 480, 7, 7)\n",
        "pool3/3x3_s2 (10, 480, 3, 3)\n",
        "pool3/3x3_s2_pool3/3x3_s2_0_split_0 (10, 480, 3, 3)\n",
        "pool3/3x3_s2_pool3/3x3_s2_0_split_1 (10, 480, 3, 3)\n",
        "pool3/3x3_s2_pool3/3x3_s2_0_split_2 (10, 480, 3, 3)\n",
        "pool3/3x3_s2_pool3/3x3_s2_0_split_3 (10, 480, 3, 3)\n",
        "inception_4a/1x1 (10, 192, 3, 3)\n",
        "inception_4a/3x3_reduce (10, 96, 3, 3)\n",
        "inception_4a/3x3 (10, 208, 3, 3)\n",
        "inception_4a/5x5_reduce (10, 16, 3, 3)\n",
        "inception_4a/5x5 (10, 48, 3, 3)\n",
        "inception_4a/pool (10, 480, 3, 3)\n",
        "inception_4a/pool_proj (10, 64, 3, 3)\n",
        "inception_4a/output (10, 512, 3, 3)\n",
        "loss1/ave_pool (10, 512, 1, 1)\n",
        "loss1/conv (10, 128, 1, 1)\n",
        "loss1/fc (10, 1024, 1, 1)\n",
        "loss1/classifier (10, 121, 1, 1)\n",
        "prob (10, 121, 1, 1)\n"
       ]
      }
     ],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}
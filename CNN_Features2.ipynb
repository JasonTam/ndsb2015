{
 "metadata": {
  "name": "",
  "signature": "sha256:62b38b4115fb9cfba7cbfb4e6cf9baf297fe22355433c7f1d47585a9837279a2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import h5py\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "import lmdb\n",
      "from caffe.proto import caffe_pb2\n",
      "import caffe\n",
      "import matplotlib.pyplot as plt\n",
      "import sys\n",
      "import tools.my_io as my_io\n",
      "%matplotlib inline\n",
      "\n",
      "\n",
      "MODEL_FILE = '/media/raid_arr/data/ndsb/config/deploy_cnn_v3_maxout_supersparse.prototxt'\n",
      "PRETRAINED = '/media/raid_arr/data/ndsb/models/zoomed_out_vanilla_smallmaxout/simple_fold0_iter_3000.caffemodel'\n",
      "\n",
      "MEAN_VALUE = 23\n",
      "\n",
      "LAYER = 'maxfc7'\n",
      "\n",
      "# N_MBATCH = 1000\n",
      "N = 10000   # Chunk size\n",
      "\n",
      "IMAGE_FILE = '/afs/ee.cooper.edu/user/t/a/tam8/data/ndsb/train/acantharia_protist/100224.jpg'\n",
      "TRAIN_DB = '/media/raid_arr/tmp/train0_norm_lmdb/'\n",
      "VAL_DB = '/media/raid_arr/tmp/test0_norm_lmdb'\n",
      "\n",
      "TRAIN_FEAT_OUT = '/media/raid_arr/data/ndsb/features/train_cnnv3_maxout_noaug.hdf5'\n",
      "VAL_FEAT_OUT = '/media/raid_arr/data/ndsb/features/val_cnnv3_maxout_noaug.hdf5'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loading From Database\n",
      "print 'Loading data...'\n",
      "tic = time.time()\n",
      "data = my_io.load_lmdb(TEST_DB)\n",
      "print \"Done in %.2f s.\" % (time.time() - tic)\n",
      "\n",
      "val_files_all, images, labels = zip(*data)\n",
      "test_labels = labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading data...\n",
        "Done in 2.40 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "image_dims = images[0].shape[:2]\n",
      "# image_dims = (57, 57)\n",
      "print image_dims\n",
      "\n",
      "net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
      "                       mean=np.array([MEAN_VALUE]),\n",
      "                       raw_scale=1.0,    # 255 if load from caffe.io, 1.0 if load from my_io lmdb\n",
      "                       image_dims=image_dims,)\n",
      "#                        gpu=True)\n",
      "# caffe.set_phase_test()\n",
      "caffe.set_mode_gpu()\n",
      "\n",
      "n_feats = net.blobs[LAYER].data.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(64, 64)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# im = caffe.io.load_image(TEST_IM, color=False)\n",
      "print 'Layer Shapes:'\n",
      "for k, v in net.blobs.items():\n",
      "    print k, v.data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Layer Shapes:\n",
        "data (10, 1, 57, 57)\n",
        "conv1 (10, 48, 26, 26)\n",
        "pool1 (10, 48, 25, 25)\n",
        "conv2 (10, 96, 21, 21)\n",
        "pool2 (10, 96, 20, 20)\n",
        "conv3 (10, 128, 20, 20)\n",
        "conv4 (10, 128, 20, 20)\n",
        "convstack_top (10, 128, 10, 10)\n",
        "fc6 (10, 2048, 1, 1)\n",
        "s1fc6 (10, 512, 1, 1)\n",
        "s2fc6 (10, 512, 1, 1)\n",
        "s3fc6 (10, 512, 1, 1)\n",
        "s4fc6 (10, 512, 1, 1)\n",
        "maxfc6 (10, 512, 1, 1)\n",
        "fc7 (10, 2048, 1, 1)\n",
        "s1fc7 (10, 512, 1, 1)\n",
        "s2fc7 (10, 512, 1, 1)\n",
        "s3fc7 (10, 512, 1, 1)\n",
        "s4fc7 (10, 512, 1, 1)\n",
        "maxfc7 (10, 512, 1, 1)\n",
        "0fc8 (10, 121, 1, 1)\n",
        "loss (10, 121, 1, 1)\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_net_feats(db_data_in, \n",
      "                  db_feat_out,\n",
      "                  layer):\n",
      "    # Create new h5 file\n",
      "    try:\n",
      "      f.close()\n",
      "    except NameError:\n",
      "      print 'Opening new db at:', db_feat_out\n",
      "\n",
      "    f = h5py.File(db_feat_out, 'w')\n",
      "\n",
      "\n",
      "    # Make Groups\n",
      "    feat_db = f.create_dataset(\"feats\", shape=(N, n_feats), maxshape=(None, n_feats), dtype='f')\n",
      "    lbls_db = f.create_dataset(\"lbls\", shape=(N,), maxshape=(None,), dtype='i8')\n",
      "    impaths_db = f.create_dataset(\"im_paths\", shape=(N,), maxshape=(None,), dtype='S120')\n",
      "\n",
      "\n",
      "    # PREDICTION TIME\n",
      "    print 'Predicting...', db_data_in\n",
      "    prediction_list = []\n",
      "    test_files_list = []\n",
      "    next_key = ''\n",
      "    first_run = True\n",
      "    while next_key or first_run:\n",
      "        print 'Starting at key: ', next_key\n",
      "        read_start = time.time()\n",
      "        data_chunk, next_key = my_io.load_lmdb_chunk(db_data_in, next_key, N)\n",
      "        print \"Read done in %.2f s.\" % (time.time() - read_start)\n",
      "        chunk_len = len(data_chunk)\n",
      "        print 'Chunk size:', chunk_len\n",
      "        sys.stdout.flush()\n",
      "        pred_start = time.time()\n",
      "\n",
      "        print 'Propagating chunks through net...'\n",
      "        sys.stdout.flush()\n",
      "        im_paths = []\n",
      "        feats = []\n",
      "        lbls = []\n",
      "        if not first_run:\n",
      "            # After the first chunk, we need to resize the db\n",
      "            feat_db.resize(feat_db.shape[0] + chunk_len, axis=0)\n",
      "            lbls_db.resize(lbls_db.shape[0] + chunk_len, axis=0)\n",
      "            impaths_db.resize(impaths_db.shape[0] + chunk_len, axis=0)\n",
      "        for ii, (im_path, im, lbl) in enumerate(data_chunk):\n",
      "            prediction = net.predict([im])\n",
      "            feat = np.squeeze(net.blobs[layer].data.mean(0))\n",
      "            feats.append(feat)\n",
      "            lbls.append(lbl)\n",
      "            im_paths.append(im_path)\n",
      "        feat_db[-chunk_len:] = np.array(feats)\n",
      "        lbls_db[-chunk_len:] = np.array(lbls)\n",
      "        impaths_db[-chunk_len:] = np.array(im_paths)\n",
      "\n",
      "\n",
      "    #     im_path_chunk, images_chunk, labels_chunk = zip(*data_chunk)\n",
      "    #     prediction = net.predict(images_chunk)\n",
      "    #     prediction_list.append(prediction)\n",
      "    #     test_files_list.append(test_files_chunk)\n",
      "        print \"Pred done in %.2f s.\" % (time.time() - pred_start)\n",
      "        sys.stdout.flush()\n",
      "        first_run = False\n",
      "\n",
      "    # predictions = np.concatenate(prediction_list)\n",
      "    # test_files = list(itertools.chain(*test_files_list))\n",
      "    print \"Done predicting\"\n",
      "    print 'DB saved in:', db_feat_out\n",
      "    return "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = time.time()\n",
      "# prediction = net.predict(images)\n",
      "get_net_feats(db_data_in=TEST_DB, \n",
      "                  db_feat_out=TEST_FEAT_OUT,\n",
      "                  layer=LAYER)\n",
      "\n",
      "print \"Done in %.2f s.\" % (time.time() - start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Opening new db at: /media/raid_arr/data/ndsb/features/test_cnnv3_maxout_noaug.hdf5\n",
        "Predicting... /media/raid_arr/tmp/test0_norm_lmdb\n",
        "Starting at key:  \n",
        "Read done in 1.78 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 6115\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 252.88 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done predicting\n",
        "Done in 254.67 s.\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = time.time()\n",
      "# prediction = net.predict(images)\n",
      "get_net_feats(db_data_in=TRAIN_DB, \n",
      "                  db_feat_out=TRAIN_FEAT_OUT,\n",
      "                  layer=LAYER)\n",
      "\n",
      "print \"Done in %.2f s.\" % (time.time() - start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Opening new db at: /media/raid_arr/data/ndsb/features/train_cnnv3_maxout_noaug.hdf5\n",
        "Predicting..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " /media/raid_arr/tmp/train0_norm_lmdb/\n",
        "Starting at key:  \n",
        "Read done in 2.62 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 10000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 419.90 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  159738.jpg\n",
        "Read done in 2.60 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 10000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 421.99 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  74744.jpg\n",
        "Read done in 1.08 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Chunk size: 4221\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pred done in 175.55 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done predicting\n",
        "Done in 1023.83 s.\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = np.array(y)\n",
      "feats_arr = np.array(feats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'y' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-38-59abedf0ad6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfeats_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import svm\n",
      "from sklearn import cross_validation\n",
      "clf = svm.SVC()\n",
      "\n",
      "scores = cross_validation.cross_val_score(clf, feats_arr, y, cv=5)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python2.7/site-packages/sklearn/cross_validation.py:413: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
        "  % (min_labels, self.n_folds)), Warning)\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 209,
       "text": [
        "[u'fc1', u'fc2', u'im_paths', u'lbls']"
       ]
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feats_arr = np.array(feats)\n",
      "feats_arr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "array([[-0.5334543 , -1.45886266, -1.85120237, ..., -0.41519433,\n",
        "        -0.2006965 , -0.43864995],\n",
        "       [-0.18853374, -0.69259894,  0.75199413, ..., -1.80381894,\n",
        "         0.55619729,  0.18945086],\n",
        "       [-1.68894136,  2.09158039,  0.10232283, ...,  1.80399609,\n",
        "         4.05035686, -0.01457913],\n",
        "       ..., \n",
        "       [-2.45586085, -1.8641001 ,  1.90774083, ..., -2.60364795,\n",
        "         1.41474581, -0.11888784],\n",
        "       [ 3.58840704, -1.30984402, -0.91113883, ..., -2.98078585,\n",
        "         3.51869011,  7.82888508],\n",
        "       [ 1.53750145,  0.28915527,  4.71641016, ...,  4.58173275,\n",
        "         0.31663072,  3.23718071]], dtype=float32)"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feats_arr.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "(6115, 512)"
       ]
      }
     ],
     "prompt_number": 30
    }
   ],
   "metadata": {}
  }
 ]
}
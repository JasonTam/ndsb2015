{
 "metadata": {
  "name": "",
  "signature": "sha256:db2ec0b213949698a257740a9f84e0211fbf808ac21a145b9e9c7343f7f5897d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Code via\n",
      "# https://github.com/benanne/kaggle-galaxies\n",
      "\n",
      "import numpy as np\n",
      "# import pandas as pd\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import layers\n",
      "import cc_layers\n",
      "import custom\n",
      "import load_data\n",
      "import realtime_augmentation as ra\n",
      "import time\n",
      "import csv\n",
      "import os\n",
      "import cPickle as pickle\n",
      "from datetime import datetime, timedelta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: GeForce GTX 580\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run copy_data_to_shm.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying /media/raid_arr/data/ndsb/train to /dev/shm...\n",
        "  took 9.38 seconds."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Copying /media/raid_arr/data/ndsb/test to /dev/shm...\n",
        "  took 45.02 seconds."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BATCH_SIZE = 16\n",
      "NUM_INPUT_FEATURES = 1 # Number of channels\n",
      "\n",
      "LEARNING_RATE_SCHEDULE = { # keyed by chunk num\n",
      "    0: 0.04,\n",
      "    1800: 0.004,\n",
      "    2300: 0.0004,\n",
      "}\n",
      "MOMENTUM = 0.9\n",
      "WEIGHT_DECAY = 0.0\n",
      "CHUNK_SIZE = 10000 # 30000 # this should be a multiple of the batch size, ideally.\n",
      "NUM_CHUNKS = 2500 # 3000 # 1500 # 600 # 600 # 600 # 500 \n",
      "VALIDATE_EVERY = 20 # 12 # 6 # 6 # 6 # 5 # validate only every 5 chunks. MUST BE A DIVISOR OF NUM_CHUNKS!!!\n",
      "# else computing the analysis data does not work correctly, since it assumes that the validation set is still loaded.\n",
      "\n",
      "NUM_CHUNKS_NONORM = 1 # train without normalisation for this many chunks, to get the weights in the right 'zone'.\n",
      "# this should be only a few, just 1 hopefully suffices.\n",
      "\n",
      "GEN_BUFFER_SIZE = 1\n",
      "\n",
      "\n",
      "TARGET_PATH = \"predictions/preds.csv\"\n",
      "ANALYSIS_PATH = \"analysis/analysis.pkl\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Set up data loading\"\n",
      "\n",
      "input_sizes = [(64, 64), (64, 64)]\n",
      "\n",
      "ds_transforms = [\n",
      "    ra.build_ds_transform(3.0, target_size=input_sizes[0]),\n",
      "    ra.build_ds_transform(3.0, target_size=input_sizes[1]) + ra.build_augmentation_transform(rotation=45)\n",
      "    ]\n",
      "\n",
      "num_input_representations = len(ds_transforms)\n",
      "\n",
      "augmentation_params = {\n",
      "    'zoom_range': (1.0 / 1.3, 1.3),\n",
      "    'rotation_range': (0, 360),\n",
      "    'shear_range': (0, 0),\n",
      "    'translation_range': (-4, 4),\n",
      "    'do_flip': True,\n",
      "}\n",
      "\n",
      "augmented_data_gen = ra.realtime_augmented_data_gen(\n",
      "    num_chunks=NUM_CHUNKS, chunk_size=CHUNK_SIZE,\n",
      "    augmentation_params=augmentation_params, ds_transforms=ds_transforms,\n",
      "    target_sizes=input_sizes)\n",
      "\n",
      "post_augmented_data_gen = ra.post_augment_brightness_gen(augmented_data_gen, std=0.5)\n",
      "\n",
      "train_gen = load_data.buffered_gen_mp(post_augmented_data_gen, buffer_size=GEN_BUFFER_SIZE)\n",
      "\n",
      "\n",
      "y_train = np.load(\"data/train_lbls.npy\")\n",
      "train_ids = load_data.train_ids\n",
      "test_ids = load_data.test_ids\n",
      "\n",
      "\n",
      "# todo: should probably use stratified K-fold (k=5 or 10)\n",
      "# split training data into training + a small validation set\n",
      "num_train = len(train_ids)\n",
      "num_test = len(test_ids)\n",
      "\n",
      "num_valid = num_train // 10 # integer division\n",
      "num_train -= num_valid\n",
      "\n",
      "y_valid = y_train[num_train:]\n",
      "y_train = y_train[:num_train]\n",
      "\n",
      "valid_ids = train_ids[num_train:]\n",
      "train_ids = train_ids[:num_train]\n",
      "\n",
      "train_indices = np.arange(num_train)\n",
      "valid_indices = np.arange(num_train, num_train + num_valid)\n",
      "test_indices = np.arange(num_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Set up data loading\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_train_gen():\n",
      "    \"\"\"\n",
      "    this generates the training data in order, for postprocessing. Do not use this for actual training.\n",
      "    \"\"\"\n",
      "    data_gen_train = ra.realtime_fixed_augmented_data_gen(train_indices, 'train',\n",
      "        ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes)\n",
      "    return load_data.buffered_gen_mp(data_gen_train, buffer_size=GEN_BUFFER_SIZE)\n",
      "\n",
      "\n",
      "def create_valid_gen():\n",
      "    data_gen_valid = ra.realtime_fixed_augmented_data_gen(valid_indices, 'train',\n",
      "        ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes)\n",
      "    return load_data.buffered_gen_mp(data_gen_valid, buffer_size=GEN_BUFFER_SIZE)\n",
      "\n",
      "\n",
      "def create_test_gen():\n",
      "    data_gen_test = ra.realtime_fixed_augmented_data_gen(test_indices, 'test',\n",
      "        ds_transforms=ds_transforms, chunk_size=CHUNK_SIZE, target_sizes=input_sizes)\n",
      "    return load_data.buffered_gen_mp(data_gen_test, buffer_size=GEN_BUFFER_SIZE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Preprocess validation data upfront\"\n",
      "start_time = time.time()\n",
      "xs_valid = [[] for _ in xrange(num_input_representations)]\n",
      "\n",
      "for data, length in create_valid_gen():\n",
      "    for x_valid_list, x_chunk in zip(xs_valid, data):\n",
      "        x_valid_list.append(x_chunk[:length])\n",
      "\n",
      "xs_valid = [np.vstack(x_valid) for x_valid in xs_valid]\n",
      "xs_valid = [x_valid.transpose(0, 3, 1, 2) for x_valid in xs_valid] # move the colour dimension up\n",
      "\n",
      "\n",
      "print \"  took %.2f seconds\" % (time.time() - start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Preprocess validation data upfront\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for data, length in create_valid_gen():\n",
      "    for x_valid_list, x_chunk in zip(xs_valid, data):\n",
      "        x_valid_list.append(x_chunk[:length])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Exception in thread Thread-4:\n",
        "Traceback (most recent call last):\n",
        "  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n",
        "    self.run()\n",
        "  File \"/usr/lib/python2.7/site-packages/zmq/utils/garbage.py\", line 37, in run\n",
        "    s.bind(self.gc.url)\n",
        "  File \"zmq/backend/cython/socket.pyx\", line 444, in zmq.backend.cython.socket.Socket.bind (zmq/backend/cython/socket.c:4092)\n",
        "  File \"zmq/backend/cython/checkrc.pxd\", line 21, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:6251)\n",
        "    raise ZMQError(errno)\n",
        "ZMQError: Address already in use\n",
        "\n",
        "Exception in thread Thread-4:\n",
        "Traceback (most recent call last):\n",
        "  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n",
        "    self.run()\n",
        "  File \"/usr/lib/python2.7/site-packages/zmq/utils/garbage.py\", line 37, in run\n",
        "    s.bind(self.gc.url)\n",
        "  File \"zmq/backend/cython/socket.pyx\", line 444, in zmq.backend.cython.socket.Socket.bind (zmq/backend/cython/socket.c:4092)\n",
        "  File \"zmq/backend/cython/checkrc.pxd\", line 21, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:6251)\n",
        "    raise ZMQError(errno)\n",
        "ZMQError: Address already in use\n",
        "\n",
        "Exception in thread Thread-4:\n",
        "Traceback (most recent call last):\n",
        "  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n",
        "    self.run()\n",
        "  File \"/usr/lib/python2.7/site-packages/zmq/utils/garbage.py\", line 37, in run\n",
        "    s.bind(self.gc.url)\n",
        "  File \"zmq/backend/cython/socket.pyx\", line 444, in zmq.backend.cython.socket.Socket.bind (zmq/backend/cython/socket.c:4092)\n",
        "  File \"zmq/backend/cython/checkrc.pxd\", line 21, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:6251)\n",
        "    raise ZMQError(errno)\n",
        "ZMQError: Address already in use\n",
        "\n",
        "Exception in thread Thread-4:\n",
        "Traceback (most recent call last):\n",
        "  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n",
        "    self.run()\n",
        "  File \"/usr/lib/python2.7/site-packages/zmq/utils/garbage.py\", line 37, in run\n",
        "    s.bind(self.gc.url)\n",
        "  File \"zmq/backend/cython/socket.pyx\", line 444, in zmq.backend.cython.socket.Socket.bind (zmq/backend/cython/socket.c:4092)\n",
        "  File \"zmq/backend/cython/checkrc.pxd\", line 21, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:6251)\n",
        "    raise ZMQError(errno)\n",
        "ZMQError: Address already in use\n",
        "\n",
        "Exception in thread Thread-4:\n",
        "Traceback (most recent call last):\n",
        "  File \"/usr/lib/python2.7/threading.py\", line 810, in __bootstrap_inner\n",
        "    self.run()\n",
        "  File \"/usr/lib/python2.7/site-packages/zmq/utils/garbage.py\", line 37, in run\n",
        "    s.bind(self.gc.url)\n",
        "  File \"zmq/backend/cython/socket.pyx\", line 444, in zmq.backend.cython.socket.Socket.bind (zmq/backend/cython/socket.c:4092)\n",
        "  File \"zmq/backend/cython/checkrc.pxd\", line 21, in zmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:6251)\n",
        "    raise ZMQError(errno)\n",
        "ZMQError: Address already in use\n",
        "\n",
        "Process Process-5:\n",
        "Traceback (most recent call last):\n",
        "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
        "    self.run()\n",
        "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
        "    self._target(*self._args, **self._kwargs)\n",
        "  File \"load_data.py\", line 564, in _buffered_generation_process\n",
        "    time.sleep(sleep_time)\n",
        "  File \"realtime_augmentation.py\", line 336, in realtime_fixed_augmented_data_gen\n",
        "    for k, imgs_aug in enumerate(gen):\n",
        "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 269, in <genexpr>\n",
        "    return (item for chunk in result for item in chunk)\n",
        "  File \"/usr/lib/python2.7/multiprocessing/pool.py\", line 659, in next\n",
        "    raise value\n",
        "IOError: [Errno 2] No such file or directory: '/dev/shm/images_train_rev1/909833.jpg'\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Build model\"\n",
      "# INPUT LAYERS\n",
      "l0 = layers.Input2DLayer(BATCH_SIZE, NUM_INPUT_FEATURES, \n",
      "                         input_sizes[0][0], input_sizes[0][1])\n",
      "l0_45 = layers.Input2DLayer(BATCH_SIZE, NUM_INPUT_FEATURES, \n",
      "                            input_sizes[1][0], input_sizes[1][1])\n",
      "\n",
      "l0r = layers.MultiRotSliceLayer([l0, l0_45], \n",
      "                                part_size=45, include_flip=True)\n",
      "\n",
      "l0s = cc_layers.ShuffleBC01ToC01BLayer(l0r) \n",
      "# CONVOLUTION & POOLING\n",
      "l1a = cc_layers.CudaConvnetConv2DLayer(l0s, \n",
      "                                       n_filters=32, filter_size=6, \n",
      "                                       weights_std=0.01, init_bias_value=0.1, \n",
      "                                       dropout=0.0, partial_sum=1, untie_biases=True)\n",
      "l1 = cc_layers.CudaConvnetPooling2DLayer(l1a, pool_size=2)\n",
      "\n",
      "l2a = cc_layers.CudaConvnetConv2DLayer(l1, \n",
      "                                       n_filters=64, filter_size=5, \n",
      "                                       weights_std=0.01, init_bias_value=0.1, \n",
      "                                       dropout=0.0, partial_sum=1, untie_biases=True)\n",
      "l2 = cc_layers.CudaConvnetPooling2DLayer(l2a, pool_size=2)\n",
      "\n",
      "l3a = cc_layers.CudaConvnetConv2DLayer(l2, \n",
      "                                       n_filters=128, filter_size=3, \n",
      "                                       weights_std=0.01, init_bias_value=0.1, \n",
      "                                       dropout=0.0, partial_sum=1, untie_biases=True)\n",
      "l3b = cc_layers.CudaConvnetConv2DLayer(l3a, \n",
      "                                       n_filters=128, filter_size=3, \n",
      "                                       pad=0, weights_std=0.1, init_bias_value=0.1, \n",
      "                                       dropout=0.0, partial_sum=1, untie_biases=True)\n",
      "l3 = cc_layers.CudaConvnetPooling2DLayer(l3b, pool_size=2)\n",
      "\n",
      "l3s = cc_layers.ShuffleC01BToBC01Layer(l3)\n",
      "\n",
      "j3 = layers.MultiRotMergeLayer(l3s, num_views=4) # 2) # merge convolutional parts\n",
      "# FULLY CONNECTED\n",
      "l4a = layers.DenseLayer(j3, \n",
      "                        n_outputs=1024, weights_std=0.001, \n",
      "                        init_bias_value=0.01, dropout=0.5, \n",
      "                        nonlinearity=layers.identity)\n",
      "l4b = layers.FeatureMaxPoolingLayer(l4a, \n",
      "                                    pool_size=2, feature_dim=1, \n",
      "                                    implementation='reshape')\n",
      "l4c = layers.DenseLayer(l4b, \n",
      "                        n_outputs=1024, weights_std=0.001, \n",
      "                        init_bias_value=0.01, dropout=0.5, \n",
      "                        nonlinearity=layers.identity)\n",
      "l4 = layers.FeatureMaxPoolingLayer(l4c, \n",
      "                                   pool_size=2, feature_dim=1, \n",
      "                                   implementation='reshape')\n",
      "\n",
      "l5 = layers.DenseLayer(l4, \n",
      "                       n_outputs=121, weights_std=0.01, \n",
      "                       init_bias_value=0.1, dropout=0.5, \n",
      "                       nonlinearity=layers.identity)\n",
      "\n",
      "# OUTPUT LAYER\n",
      "l6 = custom.OptimisedDivGalaxyOutputLayer(l5) # this incorporates the constraints on the output (probabilities sum to one, weighting, etc.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Build model\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_loss_nonorm = l6.error(normalisation=False)\n",
      "train_loss = l6.error() # but compute and print this!\n",
      "valid_loss = l6.error(dropout_active=False)\n",
      "all_parameters = layers.all_parameters(l6)\n",
      "all_bias_parameters = layers.all_bias_parameters(l6)\n",
      "\n",
      "xs_shared = [theano.shared(np.zeros((1,1,1,1), dtype=theano.config.floatX)) for _ in xrange(num_input_representations)]\n",
      "y_shared = theano.shared(np.zeros((1,1), dtype=theano.config.floatX))\n",
      "\n",
      "learning_rate = theano.shared(np.array(LEARNING_RATE_SCHEDULE[0], dtype=theano.config.floatX))\n",
      "\n",
      "idx = T.lscalar('idx')\n",
      "\n",
      "givens = {\n",
      "    l0.input_var: xs_shared[0][idx*BATCH_SIZE:(idx+1)*BATCH_SIZE],\n",
      "    l0_45.input_var: xs_shared[1][idx*BATCH_SIZE:(idx+1)*BATCH_SIZE],\n",
      "    l6.target_var: y_shared[idx*BATCH_SIZE:(idx+1)*BATCH_SIZE],\n",
      "}\n",
      "\n",
      "# updates = layers.gen_updates(train_loss, all_parameters, learning_rate=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
      "updates_nonorm = layers.gen_updates_nesterov_momentum_no_bias_decay(\n",
      "    train_loss_nonorm, all_parameters, all_bias_parameters, \n",
      "    learning_rate=learning_rate, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
      "\n",
      "updates = layers.gen_updates_nesterov_momentum_no_bias_decay(\n",
      "    train_loss, all_parameters, all_bias_parameters, \n",
      "    learning_rate=learning_rate, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
      "\n",
      "train_nonorm = theano.function([idx], train_loss_nonorm, givens=givens, updates=updates_nonorm)\n",
      "train_norm = theano.function([idx], train_loss, givens=givens, updates=updates)\n",
      "compute_loss = theano.function([idx], valid_loss, givens=givens) # dropout_active=False\n",
      "compute_output = theano.function([idx], l6.predictions(dropout_active=False), givens=givens, on_unused_input='ignore') # not using the labels, so theano complains\n",
      "compute_features = theano.function([idx], l4.output(dropout_active=False), givens=givens, on_unused_input='ignore')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Train model\"\n",
      "start_time = time.time()\n",
      "prev_time = start_time\n",
      "\n",
      "num_batches_valid = x_valid.shape[0] // BATCH_SIZE\n",
      "losses_train = []\n",
      "losses_valid = []\n",
      "\n",
      "param_stds = []\n",
      "\n",
      "for e in xrange(NUM_CHUNKS):\n",
      "    print \"Chunk %d/%d\" % (e + 1, NUM_CHUNKS)\n",
      "    chunk_data, chunk_length = train_gen.next()\n",
      "    y_chunk = chunk_data.pop() # last element is labels.\n",
      "    xs_chunk = chunk_data\n",
      "\n",
      "    # need to transpose the chunks to move the 'channels' dimension up\n",
      "    xs_chunk = [x_chunk.transpose(0, 3, 1, 2) for x_chunk in xs_chunk]\n",
      "\n",
      "    if e in LEARNING_RATE_SCHEDULE:\n",
      "        current_lr = LEARNING_RATE_SCHEDULE[e]\n",
      "        learning_rate.set_value(LEARNING_RATE_SCHEDULE[e])\n",
      "        print \"  setting learning rate to %.6f\" % current_lr\n",
      "\n",
      "    # train without normalisation for the first # chunks.\n",
      "    if e >= NUM_CHUNKS_NONORM:\n",
      "        train = train_norm\n",
      "    else:\n",
      "        train = train_nonorm\n",
      "\n",
      "    print \"  load training data onto GPU\"\n",
      "    for x_shared, x_chunk in zip(xs_shared, xs_chunk):\n",
      "        x_shared.set_value(x_chunk)\n",
      "    y_shared.set_value(y_chunk)\n",
      "    num_batches_chunk = x_chunk.shape[0] // BATCH_SIZE\n",
      "\n",
      "    print \"  batch SGD\"\n",
      "    losses = []\n",
      "    for b in xrange(num_batches_chunk):\n",
      "        # if b % 1000 == 0:\n",
      "        #     print \"  batch %d/%d\" % (b + 1, num_batches_chunk)\n",
      "\n",
      "        loss = train(b)\n",
      "        losses.append(loss)\n",
      "        # print \"  loss: %.6f\" % loss\n",
      "\n",
      "    mean_train_loss = np.sqrt(np.mean(losses))\n",
      "    print \"  mean training loss (RMSE):\\t\\t%.6f\" % mean_train_loss\n",
      "    losses_train.append(mean_train_loss)\n",
      "\n",
      "    # store param stds during training\n",
      "    param_stds.append([p.std() for p in layers.get_param_values(l6)])\n",
      "\n",
      "    if ((e + 1) % VALIDATE_EVERY) == 0:\n",
      "        print\n",
      "        print \"VALIDATING\"\n",
      "        print \"  load validation data onto GPU\"\n",
      "        for x_shared, x_valid in zip(xs_shared, xs_valid):\n",
      "            x_shared.set_value(x_valid)\n",
      "        y_shared.set_value(y_valid)\n",
      "\n",
      "        print \"  compute losses\"\n",
      "        losses = []\n",
      "        for b in xrange(num_batches_valid):\n",
      "            # if b % 1000 == 0:\n",
      "            #     print \"  batch %d/%d\" % (b + 1, num_batches_valid)\n",
      "            loss = compute_loss(b)\n",
      "            losses.append(loss)\n",
      "\n",
      "        mean_valid_loss = np.sqrt(np.mean(losses))\n",
      "        print \"  mean validation loss (RMSE):\\t\\t%.6f\" % mean_valid_loss\n",
      "        losses_valid.append(mean_valid_loss)\n",
      "\n",
      "    now = time.time()\n",
      "    time_since_start = now - start_time\n",
      "    time_since_prev = now - prev_time\n",
      "    prev_time = now\n",
      "    est_time_left = time_since_start * (float(NUM_CHUNKS - (e + 1)) / float(e + 1))\n",
      "    eta = datetime.now() + timedelta(seconds=est_time_left)\n",
      "    eta_str = eta.strftime(\"%c\")\n",
      "    print \"  %s since start (%.2f s)\" % (load_data.hms(time_since_start), time_since_prev)\n",
      "    print \"  estimated %s to go (ETA: %s)\" % (load_data.hms(est_time_left), eta_str)\n",
      "    print\n",
      "\n",
      "\n",
      "del chunk_data, xs_chunk, x_chunk, y_chunk, xs_valid, x_valid # memory cleanup"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Compute predictions on validation set for analysis in batches\"\n",
      "predictions_list = []\n",
      "for b in xrange(num_batches_valid):\n",
      "    # if b % 1000 == 0:\n",
      "    #     print \"  batch %d/%d\" % (b + 1, num_batches_valid)\n",
      "\n",
      "    predictions = compute_output(b)\n",
      "    predictions_list.append(predictions)\n",
      "\n",
      "all_predictions = np.vstack(predictions_list)\n",
      "\n",
      "# postprocessing: clip all predictions to 0-1\n",
      "all_predictions[all_predictions > 1] = 1.0\n",
      "all_predictions[all_predictions < 0] = 0.0\n",
      "\n",
      "print \"Write validation set predictions to %s\" % ANALYSIS_PATH\n",
      "with open(ANALYSIS_PATH, 'w') as f:\n",
      "    pickle.dump({\n",
      "        'ids': valid_ids[:num_batches_valid * BATCH_SIZE], # note that we need to truncate the ids to a multiple of the batch size.\n",
      "        'predictions': all_predictions,\n",
      "        'targets': y_valid,\n",
      "        'mean_train_loss': mean_train_loss,\n",
      "        'mean_valid_loss': mean_valid_loss,\n",
      "        'time_since_start': time_since_start,\n",
      "        'losses_train': losses_train,\n",
      "        'losses_valid': losses_valid,\n",
      "        'param_values': layers.get_param_values(l6),\n",
      "        'param_stds': param_stds,\n",
      "    }, f, pickle.HIGHEST_PROTOCOL)\n",
      "\n",
      "\n",
      "del predictions_list, all_predictions # memory cleanup\n",
      "\n",
      "\n",
      "print \"Computing predictions on test data\"\n",
      "predictions_list = []\n",
      "for e, (xs_chunk, chunk_length) in enumerate(create_test_gen()):\n",
      "    print \"Chunk %d\" % (e + 1)\n",
      "    xs_chunk = [x_chunk.transpose(0, 3, 1, 2) for x_chunk in xs_chunk] # move the colour dimension up.\n",
      "\n",
      "    for x_shared, x_chunk in zip(xs_shared, xs_chunk):\n",
      "        x_shared.set_value(x_chunk)\n",
      "\n",
      "    num_batches_chunk = int(np.ceil(chunk_length / float(BATCH_SIZE)))  # need to round UP this time to account for all data\n",
      "\n",
      "    # make predictions for testset, don't forget to cute off the zeros at the end\n",
      "    for b in xrange(num_batches_chunk):\n",
      "        # if b % 1000 == 0:\n",
      "        #     print \"  batch %d/%d\" % (b + 1, num_batches_chunk)\n",
      "\n",
      "        predictions = compute_output(b)\n",
      "        predictions_list.append(predictions)\n",
      "\n",
      "\n",
      "all_predictions = np.vstack(predictions_list)\n",
      "all_predictions = all_predictions[:num_test] # truncate back to the correct length\n",
      "\n",
      "# postprocessing: clip all predictions to 0-1\n",
      "all_predictions[all_predictions > 1] = 1.0\n",
      "all_predictions[all_predictions < 0] = 0.0\n",
      "\n",
      "\n",
      "print \"Write predictions to %s\" % TARGET_PATH\n",
      "# test_ids = np.load(\"data/test_ids.npy\")\n",
      "\n",
      "with open(TARGET_PATH, 'wb') as csvfile:\n",
      "    writer = csv.writer(csvfile) # , delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
      "\n",
      "    # write header\n",
      "    writer.writerow(['GalaxyID', 'Class1.1', 'Class1.2', 'Class1.3', 'Class2.1', 'Class2.2', 'Class3.1', 'Class3.2', 'Class4.1', 'Class4.2', 'Class5.1', 'Class5.2', 'Class5.3', 'Class5.4', 'Class6.1', 'Class6.2', 'Class7.1', 'Class7.2', 'Class7.3', 'Class8.1', 'Class8.2', 'Class8.3', 'Class8.4', 'Class8.5', 'Class8.6', 'Class8.7', 'Class9.1', 'Class9.2', 'Class9.3', 'Class10.1', 'Class10.2', 'Class10.3', 'Class11.1', 'Class11.2', 'Class11.3', 'Class11.4', 'Class11.5', 'Class11.6'])\n",
      "\n",
      "    # write data\n",
      "    for k in xrange(test_ids.shape[0]):\n",
      "        row = [test_ids[k]] + all_predictions[k].tolist()\n",
      "        writer.writerow(row)\n",
      "\n",
      "print \"Gzipping...\"\n",
      "os.system(\"gzip -c %s > %s.gz\" % (TARGET_PATH, TARGET_PATH))\n",
      "\n",
      "\n",
      "del all_predictions, predictions_list, xs_chunk, x_chunk # memory cleanup\n",
      "\n",
      "\n",
      "print \"Done!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": "",
  "signature": "sha256:73e9d8cff3c6615ba096dbb7ba769bd305a0bfcf9aa1be59b7fba076e8e9d041"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Try predicting the augmented versions as well\n",
      "# aggregate the results from all augmentations of 1 image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import time\n",
      "import sys\n",
      "import tools.my_io as my_io\n",
      "import caffe\n",
      "import itertools\n",
      "import pickle\n",
      "\n",
      "# Set the right path to your model definition file, pretrained model weights,\n",
      "# and the image you would like to classify.\n",
      "MODEL_FILE = './deploy_vanilla.prototxt'\n",
      "# PRETRAINED = './models/vanilla/vanilla_iter_20000.caffemodel'\n",
      "PRETRAINED = '/media/raid_arr/data/ndsb/models/pl_iter_38000.caffemodel'\n",
      "MEAN_FILE = '/media/raid_arr/data/ndsb/augment/testaug_mean.npy'\n",
      "# TEST_FILE = './data/test_final.txt'\n",
      "# TEST_DB = '/data/ndsb/ndsb_test_lmdb'\n",
      "# TEST_DB = '/data/ndsb/augment/ndsb_testaug_lmdb/'\n",
      "TEST_DB = './data/64x64/ndsb_test_lmdb'\n",
      "N = 1000   # Chunk size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # Loading From Database\n",
      "# reload(my_io)\n",
      "# print 'Loading data...'\n",
      "# tic = time.time()\n",
      "# data, next_key = my_io.load_lmdb_chunk(TEST_DB, '', 1000000)\n",
      "# print \"Done in %.2f s.\" % (time.time() - tic)\n",
      "\n",
      "# # test_files_all, images, labels = zip(*data)\n",
      "# # test_labels = labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading data...\n",
        "Done in 0.95 s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# image_dims = data[0][1].shape[:2]\n",
      "image_dims = (64, 64)\n",
      "net = caffe.Classifier(MODEL_FILE, PRETRAINED,\n",
      "                       mean=np.load(MEAN_FILE),\n",
      "                       raw_scale=1.0,    # 255 if load from caffe.io, 1.0 if load from my_io lmdb\n",
      "                       image_dims=image_dims,\n",
      "                       gpu=True)\n",
      "caffe.set_phase_test()\n",
      "caffe.set_mode_gpu()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # Chunking for the sake of memory\n",
      "# def chunks(l, n):\n",
      "#     \"\"\" Yield successive n-sized chunks from l.\n",
      "#     \"\"\"\n",
      "#     #for i in xrange(0, len(l), n):\n",
      "#     #    yield l[i:i+n]\n",
      "#     return [l[i:i+n] for i in xrange(0, len(l), n)]\n",
      "\n",
      "# data_chunks = chunks(data, N)\n",
      "# len(data_chunks)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "7"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(my_io)\n",
      "# PREDICTION TIME\n",
      "# print 'Predicting...', len(data), 'images'\n",
      "prediction_list = []\n",
      "test_files_list = []\n",
      "next_key = ''\n",
      "# for ii, data_chunk in enumerate(data_chunks):\n",
      "first_run = True\n",
      "while next_key or first_run:\n",
      "    print 'Starting at key: ', next_key\n",
      "    \n",
      "    first_run = False\n",
      "    data_chunk, next_key = my_io.load_lmdb_chunk(TEST_DB, next_key, N)\n",
      "    print 'Chunk size:', len(data_chunk)\n",
      "    sys.stdout.flush()\n",
      "    start = time.time()\n",
      "    test_files_chunk, images_chunk, fake_labels = zip(*data_chunk)\n",
      "#     prediction = net.predict(images_chunk)\n",
      "    prediction = np.array([1])\n",
      "    prediction_list.append(prediction)\n",
      "    test_files_list.append(test_files_chunk)\n",
      "    print \"Done in %.2f s.\" % (time.time() - start)\n",
      "    sys.stdout.flush()\n",
      "    \n",
      "predictions = np.concatenate(prediction_list)\n",
      "test_files = list(itertools.chain(*test_files_list))\n",
      "print \"Done predicting\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  \n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 0.00 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00001000_/afs/ee.cooper.edu/user/t/a/tam8/data/ndsb/train/diatom_chain_tube/103660.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 0.00 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00002000_/afs/ee.cooper.edu/user/t/a/tam8/data/ndsb/train/copepod_calanoid_large/103657.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 0.00 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00003000_/afs/ee.cooper.edu/user/t/a/tam8/data/ndsb/train/protist_other/112696.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 0.00 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00004000_/afs/ee.cooper.edu/user/t/a/tam8/data/ndsb/train/hydromedusae_shapeA_sideview_small/103688.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 0.00 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00005000_/afs/ee.cooper.edu/user/t/a/tam8/data/ndsb/train/protist_other/103790.jpg\n",
        "Chunk size:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 0.00 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting at key:  00006000_/afs/ee.cooper.edu/user/t/a/tam8/data/ndsb/train/trichodesmium_puff/110777.jpg\n",
        "Chunk size: 115\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done in 0.00 s.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done predicting\n"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(test_files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 106,
       "text": [
        "6115"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(predictions, open('/media/raid_arr/data/ndsb/saved_preds/pred.p', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SUBMISSION CREATION\n",
      "test_files_all, images, labels = zip(*data)\n",
      "\n",
      "import tools.submission as sub\n",
      "f_name='SUBMISSION_PL38000_LEGIT.csv'\n",
      "sub.make_submission(test_files_all, predictions, f_name=f_name)\n",
      "\n",
      "print 'Submission created:', f_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Submission created: SUBMISSION_PL38000_LEGIT.csv\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "130400"
       ]
      }
     ],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}